{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom keras import backend as K\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Input\nfrom keras.layers import LSTM,GRU\nfrom keras.layers import BatchNormalization,Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/smemi309-final-evaluation-challenge-2021/test14slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test5slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train1slice.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train4slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test15slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train8slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train15slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test6slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test7slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train2slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test4slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test2slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train6slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/sampleSubmission.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test10slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train10slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test11slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train7slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train14slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train5slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test3slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test12slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test9slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train3slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test1slice.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train12slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test8slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train13slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train9slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/test13slices.csv\n/kaggle/input/smemi309-final-evaluation-challenge-2021/train11slices.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading the Datasets","metadata":{}},{"cell_type":"code","source":"train_1 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train1slice.csv\")\ntrain_2 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train2slices.csv\")\ntrain_3 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train3slices.csv\")\ntrain_4 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train4slices.csv\")\ntrain_5 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train5slices.csv\")\ntrain_6 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train6slices.csv\")\ntrain_7 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train7slices.csv\")\ntrain_8 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train8slices.csv\")\ntrain_9 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train9slices.csv\")\ntrain_10 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train10slices.csv\")\n\n\ntest_1 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test1slice.csv\")\ntest_2 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test2slices.csv\")\ntest_3 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test3slices.csv\")\ntest_4 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test4slices.csv\")\ntest_5 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test5slices.csv\")\ntest_6 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test6slices.csv\")\ntest_7 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test7slices.csv\")\ntest_8 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test8slices.csv\")\ntest_9 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test9slices.csv\")\ntest_10 = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test10slices.csv\")\n\nprint(\"done\")","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  **Approach 1**: Using SKLEARN Classifiers","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nimport time\nimport matplotlib.pyplot as plt\nimport scipy.stats as stat\n\nprint(sklearn.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:03:16.24724Z","iopub.execute_input":"2021-11-29T13:03:16.24769Z","iopub.status.idle":"2021-11-29T13:03:16.696018Z","shell.execute_reply.started":"2021-11-29T13:03:16.247654Z","shell.execute_reply":"2021-11-29T13:03:16.694916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grid Search ","metadata":{}},{"cell_type":"markdown","source":"We use the Grid Search to find the best hyperparameters for each model. Below is an example on the Adaboost Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\n\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:09:42.208303Z","iopub.execute_input":"2021-11-29T14:09:42.209137Z","iopub.status.idle":"2021-11-29T14:09:42.21511Z","shell.execute_reply.started":"2021-11-29T14:09:42.209085Z","shell.execute_reply":"2021-11-29T14:09:42.213352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\ncrossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\nfor depth in range (1,10):\n    tree_classifier=tree.DecisionTreeClassifier(max_depth=depth,random_state=1)\n    if tree_classifier.fit(X_train,Y_train).tree_.max_depth<depth:\n        break\n    score=np.mean(cross_val_score(tree_classifier,X_train,Y_train,scoring='accuracy', cv=crossvalidation,n_jobs=1))\n    print(depth, score)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:10:22.276445Z","iopub.execute_input":"2021-11-29T14:10:22.276751Z","iopub.status.idle":"2021-11-29T14:10:26.97658Z","shell.execute_reply.started":"2021-11-29T14:10:22.276718Z","shell.execute_reply":"2021-11-29T14:10:26.974847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada=AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=None))\nsearch_grid={'n_estimators':[100,150,200,250],'base_estimator__max_depth':[1,5,15,25,27],\"base_estimator__criterion\" : [\"gini\", \"entropy\"]}\nsearch=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='accuracy',n_jobs=1,cv=crossvalidation,verbose = 5)\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:52:03.815778Z","iopub.execute_input":"2021-11-29T14:52:03.816708Z","iopub.status.idle":"2021-11-29T14:52:03.825109Z","shell.execute_reply.started":"2021-11-29T14:52:03.816653Z","shell.execute_reply":"2021-11-29T14:52:03.823757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada.get_params().keys()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T12:59:07.585906Z","iopub.execute_input":"2021-11-27T12:59:07.586166Z","iopub.status.idle":"2021-11-27T12:59:07.591559Z","shell.execute_reply.started":"2021-11-27T12:59:07.586137Z","shell.execute_reply":"2021-11-27T12:59:07.590832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train_1.drop('label',axis=1)\ny_train = train_1['label']","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:52:06.199296Z","iopub.execute_input":"2021-11-29T14:52:06.199614Z","iopub.status.idle":"2021-11-29T14:52:06.208835Z","shell.execute_reply.started":"2021-11-29T14:52:06.199582Z","shell.execute_reply":"2021-11-29T14:52:06.207863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search.fit(x_train,y_train)\nsearch.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:52:07.979491Z","iopub.execute_input":"2021-11-29T14:52:07.980687Z","iopub.status.idle":"2021-11-29T14:57:31.576799Z","shell.execute_reply.started":"2021-11-29T14:52:07.980633Z","shell.execute_reply":"2021-11-29T14:57:31.575775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grid Search for Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"Grad=GradientBoostingClassifier(random_state=0, learning_rate=0.01)\nsearch_grid={'n_estimators':[100,150,200,250],'max_depth':[5,9,15,25,30]}\nsearch2=GridSearchCV(estimator=Grad,param_grid=search_grid,scoring='accuracy',n_jobs=1,cv=crossvalidation)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:13:48.696163Z","iopub.execute_input":"2021-11-27T14:13:48.696761Z","iopub.status.idle":"2021-11-27T14:13:48.701347Z","shell.execute_reply.started":"2021-11-27T14:13:48.696722Z","shell.execute_reply":"2021-11-27T14:13:48.700671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Grad.get_params().keys()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:13:50.994355Z","iopub.execute_input":"2021-11-27T14:13:50.995269Z","iopub.status.idle":"2021-11-27T14:13:51.002262Z","shell.execute_reply.started":"2021-11-27T14:13:50.995236Z","shell.execute_reply":"2021-11-27T14:13:51.001279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search2.fit(X_train,Y_train)\nsearch2.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Figuring the Best Model\n","metadata":{}},{"cell_type":"code","source":"#list of classifiers to test\nrandom_forest = RandomForestClassifier(n_estimators=180, max_depth=5)\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=9), n_estimators=180)\nadaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=180)\ngradient = GradientBoostingClassifier(n_estimators=180, learning_rate=1.0,max_depth=5, random_state=0)\nbagging_neighbors = BaggingClassifier(KNeighborsClassifier(), max_samples=0.8, max_features=0.5)\n\nxgb1 = XGBClassifier(learning_rate =0.1,n_estimators=100,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,\n                     colsample_bytree=0.8,objective= 'multi:softmax',nthread=4,seed=27)\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:03:29.277683Z","iopub.execute_input":"2021-11-29T13:03:29.278018Z","iopub.status.idle":"2021-11-29T13:03:29.290174Z","shell.execute_reply.started":"2021-11-29T13:03:29.277986Z","shell.execute_reply":"2021-11-29T13:03:29.288946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"change \"train_2\" to the corresponding number of slices you want \n\n(except for slice 5 because it doesnt have labelled testing dataset//for slice 5 we split the train data into training and validation)\n","metadata":{}},{"cell_type":"code","source":"x_train = train_2.drop('label',axis=1)\ny_train = train_2['label']\n\nX_test = test_2.drop('label', axis=1)\nY_test = test_2['label']\n\n#Use for slice 5\n#X_train, X_val, Y_train, Y_val = train_test_split(x_train,y_train, test_size=0.25, shuffle=True)\n\n\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:47:38.36148Z","iopub.execute_input":"2021-11-29T13:47:38.362378Z","iopub.status.idle":"2021-11-29T13:47:38.377748Z","shell.execute_reply.started":"2021-11-29T13:47:38.362339Z","shell.execute_reply":"2021-11-29T13:47:38.376728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = [random_forest, bagging, xgb1, gradient, bagging_neighbors, adaboost]\nAccuracies = []\nScores = []\n\n\nfor model in model:\n        start = time.time()\n        model.fit(x_train, y_train)\n        stop = time.time()\n        #Use x_val and Y_val or X_test and Y_test\n        perf=model.score(X_test,Y_test)\n        Scores.append(perf)\n        sstart = time.time()\n        predicted = model.predict(X_test)\n        sstop = time.time()\n        Accuracies.append(accuracy_score(Y_test, predicted))\n        print(f\"Training time of classifier: {stop - start}s\", \"with an accuracy of\" , perf, f\"and an inference time of : {sstop-sstart}s\")\n        #disp = metrics.plot_confusion_matrix(model, X_val, Y_val)\n        #disp.figure_.suptitle(\"Confusion Matrix \")\n        #plt.show()\nprint(\"done\")    ","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:50:28.217411Z","iopub.execute_input":"2021-11-29T13:50:28.217766Z","iopub.status.idle":"2021-11-29T13:52:59.595082Z","shell.execute_reply.started":"2021-11-29T13:50:28.217723Z","shell.execute_reply":"2021-11-29T13:52:59.594077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Accuracies)\nprint(len(Accuracies))\n\nprint(Scores)\nprint(len(Scores))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T21:49:09.06093Z","iopub.execute_input":"2021-11-28T21:49:09.061367Z","iopub.status.idle":"2021-11-28T21:49:09.070897Z","shell.execute_reply.started":"2021-11-28T21:49:09.061331Z","shell.execute_reply":"2021-11-28T21:49:09.06991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nmodels = ['random_forest', 'bagging', 'xgb1', 'gradient', 'bagging_neighbors', 'adaboost']\nax.bar(models,Accuracies)\n#ax.yaxis.set_major_formatter(FormatStrFormatter('%g'))\nax.yaxis.set_ticks(np.arange(0.0,0.9, 0.05))\nax.set_xlabel('Models') # Label x-axis\nax.set_ylabel('Total Accuracy for 2 Slices') # Label y-axis\nax.set_title('Accuracy for 2 Slices')\nplt.show()\n\nprint(\"The maximum accuracy for 2 slices is:  \"+ str(np.amax(Accuracies)))\nindex=np.argmax(Accuracies)\nprint(\"It corresponds to the model:  \"+ models[index])","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:53:56.677015Z","iopub.execute_input":"2021-11-29T13:53:56.677966Z","iopub.status.idle":"2021-11-29T13:53:56.926655Z","shell.execute_reply.started":"2021-11-29T13:53:56.67791Z","shell.execute_reply":"2021-11-29T13:53:56.92557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy vs slice","metadata":{}},{"cell_type":"markdown","source":"In the previous part, the best classifier with highest accuracy was the BaggingN classifier.\nSo we use it in this is the code to plot the accuracy vs number of slices graph and analyze the results","metadata":{}},{"cell_type":"code","source":"x_train = train_3.drop('label',axis=1)\ny_train = train_3['label']\n\n\n\nX_test = test_3.drop('label', axis=1)\nY_test = test_3['label']\n\n\nX_train, X_val, Y_train, Y_val = train_test_split(x_train,y_train, test_size=0.25, shuffle=True)\n\n\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:28:40.556157Z","iopub.execute_input":"2021-11-29T15:28:40.556492Z","iopub.status.idle":"2021-11-29T15:28:40.570929Z","shell.execute_reply.started":"2021-11-29T15:28:40.556459Z","shell.execute_reply":"2021-11-29T15:28:40.569695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#Best model\n#classifier = GradientBoostingClassifier(n_estimators=180, learning_rate=1.0,max_depth=5, random_state=0)\nbaseL= LogisticRegression(C=1.0,class_weight=None,dual=False,fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n          verbose=0, warm_start=False)\n\nbaseD=DecisionTreeClassifier(class_weight=None,criterion='gini',max_depth=15,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0,  random_state=None,\n            splitter='best')\n\n\nclassifier = BaggingClassifier(KNeighborsClassifier(), max_samples=0.8, max_features=0.5)\nstart = time.time()\nclassifier.fit(X_train, Y_train)\nstop = time.time()\nperf=classifier.score(X_val,Y_val)\nsstart = time.time()\n#predicted = classifier.predict(test_5_slice)\nsstop = time.time()\nprint(f\"Training time of classifier: {stop - start}s\", \"with an accuracy of\" , perf, f\"and an inference time of : {sstop-sstart}s\")\ndisp = metrics.plot_confusion_matrix(classifier, X_val, Y_val)\ndisp.figure_.suptitle(\"Confusion Matrix \")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:28:59.392711Z","iopub.execute_input":"2021-11-29T15:28:59.3933Z","iopub.status.idle":"2021-11-29T15:29:00.847706Z","shell.execute_reply.started":"2021-11-29T15:28:59.393264Z","shell.execute_reply":"2021-11-29T15:29:00.846756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These Accuracy arrays where obtained after running the code over all the slices accordingly","metadata":{}},{"cell_type":"code","source":"xvals = [1,2,3,4,5,6,7,8,9,10]\nAccuracie = [58.33,64.01,68.88,68.56,68.88,70.07,70.45,70.37,71.12,73.33]\n\nf, ax = plt.subplots(1)\nax.plot(xvals, Accuracie,marker='o')\nax.yaxis.set_ticks(np.arange(64,75, 2))\nax.xaxis.set_ticks(np.arange(1,11, 1))\nfor i,j in zip(xvals,Accuracie):\n    ax.annotate(str(j),xy=(i,j+0.5))\nax.set_xlabel('Number of Slices') # Label x-axis\nax.set_ylabel('Total Accuracy') # Label y-axis\nax.set_title('Accuracy VS Nb_Slices for BaggingN Classifier')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:21:43.159824Z","iopub.execute_input":"2021-11-29T22:21:43.160529Z","iopub.status.idle":"2021-11-29T22:21:43.367767Z","shell.execute_reply.started":"2021-11-29T22:21:43.160491Z","shell.execute_reply":"2021-11-29T22:21:43.367037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make Submission","metadata":{}},{"cell_type":"code","source":"predicted = model.predict(X_test)\nprint(predicted)\nids= list(range(0, 264))\nsubmission = pd.DataFrame({'id':ids,'label':predicted})\nsubmission.to_csv(\"sampleSubmissionBagging.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T21:55:17.874754Z","iopub.execute_input":"2021-11-28T21:55:17.875009Z","iopub.status.idle":"2021-11-28T21:55:18.384802Z","shell.execute_reply.started":"2021-11-28T21:55:17.874981Z","shell.execute_reply":"2021-11-28T21:55:18.38316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Approach 2**: Using Neural Networks","metadata":{}},{"cell_type":"markdown","source":"# 2.1 2D CNN Model","metadata":{}},{"cell_type":"markdown","source":"Load Training and Testing Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train5slices.csv\").values\n\ntest= pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test5slices.csv\").values\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T22:45:19.985975Z","iopub.execute_input":"2021-11-30T22:45:19.986809Z","iopub.status.idle":"2021-11-30T22:45:20.077509Z","shell.execute_reply.started":"2021-11-30T22:45:19.986767Z","shell.execute_reply":"2021-11-30T22:45:20.076718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reshape the data for 2D convolution","metadata":{}},{"cell_type":"code","source":"# Reshape training data\nX_train = train[:, 1:].reshape(train.shape[0],1,24,24).astype( 'float32' )\ny_train = train[:,0]\n\n\n# Reshape test data\nX_test = test[:,1:].reshape(test.shape[0],1,24,24).astype( 'float32' )\ny_test = test[:,0]\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:16:51.25111Z","iopub.execute_input":"2021-11-29T22:16:51.251392Z","iopub.status.idle":"2021-11-29T22:16:51.258709Z","shell.execute_reply.started":"2021-11-29T22:16:51.251343Z","shell.execute_reply":"2021-11-29T22:16:51.257785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D\n\nmodel = Sequential()\n#K.set_image_dim_ordering('th') which is channels first\nK.set_image_data_format('channels_first')\nmodel.add(Conv2D(64, (3, 3), input_shape=(1, 24,24), activation=\"relu\", padding=\"valid\"))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Conv2D(32, 3, 3, activation= 'relu' ))\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation= 'relu' ))\nmodel.add(Dense(128, activation= 'relu' ))\nmodel.add(Dense(64, activation= 'relu' ))\nmodel.add(Dense(11, activation= 'softmax' ))\n  # Compile model\nmodel.compile(loss= 'SparseCategoricalCrossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:16:57.80575Z","iopub.execute_input":"2021-11-29T22:16:57.806035Z","iopub.status.idle":"2021-11-29T22:16:57.883338Z","shell.execute_reply.started":"2021-11-29T22:16:57.806005Z","shell.execute_reply":"2021-11-29T22:16:57.882632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:13:48.972763Z","iopub.execute_input":"2021-11-29T22:13:48.973047Z","iopub.status.idle":"2021-11-29T22:13:48.987716Z","shell.execute_reply.started":"2021-11-29T22:13:48.973016Z","shell.execute_reply":"2021-11-29T22:13:48.98694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train,\n          epochs=20,\n          batch_size= 8)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:17:01.172591Z","iopub.execute_input":"2021-11-29T22:17:01.17325Z","iopub.status.idle":"2021-11-29T22:17:10.36407Z","shell.execute_reply.started":"2021-11-29T22:17:01.173211Z","shell.execute_reply":"2021-11-29T22:17:10.363211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, batch_size=8)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:17:13.118422Z","iopub.execute_input":"2021-11-29T22:17:13.119136Z","iopub.status.idle":"2021-11-29T22:17:13.348134Z","shell.execute_reply.started":"2021-11-29T22:17:13.119092Z","shell.execute_reply":"2021-11-29T22:17:13.347351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xvals = [1,4,9]\nAccuracie = [58.33,58.71,67.42]\n\nf, ax = plt.subplots(1)\nax.plot(xvals, Accuracie,marker='o')\nax.yaxis.set_ticks(np.arange(57,70, 2))\nax.xaxis.set_ticks(np.arange(1,11, 1))\nfor i,j in zip(xvals,Accuracie):\n    ax.annotate(str(j),xy=(i,j+0.5))\nax.set_xlabel('Number of Slices') # Label x-axis\nax.set_ylabel('Total Accuracy') # Label y-axis\nax.set_title('Accuracy VS Nb_Slices for Conv2D Model')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T22:24:22.360859Z","iopub.execute_input":"2021-11-29T22:24:22.361136Z","iopub.status.idle":"2021-11-29T22:24:22.539618Z","shell.execute_reply.started":"2021-11-29T22:24:22.361105Z","shell.execute_reply":"2021-11-29T22:24:22.538869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 Using Conv1D with LSTM/GRU","metadata":{}},{"cell_type":"markdown","source":" Load and Reshape the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/train2slices.csv\").values\n\ntest= pd.read_csv(\"/kaggle/input/smemi309-final-evaluation-challenge-2021/test2slices.csv\").values\n\nprint(\"done\")","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"#Multiply the 64 by the number of slices and change in (64,1)\nX_train =train[:, 1:].reshape(train.shape[0], 128,1).astype('float32')\ny_train = train[:,0]\n\n\nX_test = test[:, 1:].reshape(test.shape[0],128,1).astype('float32')\ny_test = test[:,0]\n\n#this is specific for slice 5\n#X_test = test.reshape(test.shape[0], 320,1).astype('float32')\nprint(\"done\")","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_train[1].shape)\nprint(y_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:46:45.347417Z","iopub.execute_input":"2021-11-30T20:46:45.349852Z","iopub.status.idle":"2021-11-30T20:46:45.35571Z","shell.execute_reply.started":"2021-11-30T20:46:45.349809Z","shell.execute_reply":"2021-11-30T20:46:45.354932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nTrainx, Valx, Trainy, Valy = train_test_split(X_train,y_train, test_size=0.25, random_state=42)\nprint(\"done\")","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model 1 :Conv1D with 1 GRU layer","metadata":{}},{"cell_type":"code","source":"num_classes=11\nfrom keras.layers import GRU\nfrom keras.layers import Activation\n\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, input_shape=Trainx[1].shape))\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n\nmodel.add(GRU(64, return_sequences=True,recurrent_dropout=0.5))\n\n\n\n#model.add(Dropout(0.5))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax')) \n\nmodel.compile(loss= 'sparse_categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n\nprint(\"done\")","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model 2:Conv1D with 1 LSTM layer","metadata":{}},{"cell_type":"code","source":"num_classes=11\nfrom keras.layers import LSTM\nfrom keras.layers import Activation\n\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, input_shape=Trainx[1].shape))\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n\nmodel.add(LSTM(64, return_sequences=True,recurrent_dropout=0.5))\n\n\n\n#model.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax')) \n\nmodel.compile(loss= 'sparse_categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n\nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:47:40.41864Z","iopub.execute_input":"2021-11-30T20:47:40.419447Z","iopub.status.idle":"2021-11-30T20:47:40.571893Z","shell.execute_reply.started":"2021-11-30T20:47:40.419398Z","shell.execute_reply":"2021-11-30T20:47:40.571092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using Early Stopping to avoid overfitiitng\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n\nmodel.fit(Trainx, Trainy,\n          epochs=20,\n          batch_size= 128,validation_data=(Valx, Valy),callbacks=[callback])\n","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2021-12-01 22:03:09.775119: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2021-12-01 22:03:12.354668: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"7/7 [==============================] - 11s 444ms/step - loss: 2.2905 - accuracy: 0.1760 - val_loss: 2.0159 - val_accuracy: 0.3296\nEpoch 2/20\n7/7 [==============================] - 3s 383ms/step - loss: 1.7664 - accuracy: 0.3779 - val_loss: 1.4627 - val_accuracy: 0.4667\nEpoch 3/20\n7/7 [==============================] - 3s 474ms/step - loss: 1.3920 - accuracy: 0.4895 - val_loss: 1.4039 - val_accuracy: 0.4704\nEpoch 4/20\n7/7 [==============================] - 3s 382ms/step - loss: 1.2683 - accuracy: 0.5403 - val_loss: 1.2351 - val_accuracy: 0.5296\nEpoch 5/20\n7/7 [==============================] - 3s 392ms/step - loss: 1.1322 - accuracy: 0.5985 - val_loss: 1.1290 - val_accuracy: 0.5556\nEpoch 6/20\n7/7 [==============================] - 3s 501ms/step - loss: 1.0263 - accuracy: 0.6010 - val_loss: 1.1671 - val_accuracy: 0.5333\nEpoch 7/20\n7/7 [==============================] - 3s 458ms/step - loss: 0.9799 - accuracy: 0.6332 - val_loss: 1.0629 - val_accuracy: 0.5963\nEpoch 8/20\n7/7 [==============================] - 3s 451ms/step - loss: 0.9443 - accuracy: 0.6419 - val_loss: 1.1287 - val_accuracy: 0.5519\nEpoch 9/20\n7/7 [==============================] - 3s 417ms/step - loss: 0.9027 - accuracy: 0.6357 - val_loss: 0.9978 - val_accuracy: 0.6296\nEpoch 10/20\n7/7 [==============================] - 3s 469ms/step - loss: 0.8139 - accuracy: 0.7001 - val_loss: 0.9497 - val_accuracy: 0.6333\nEpoch 11/20\n7/7 [==============================] - 3s 404ms/step - loss: 0.7432 - accuracy: 0.7323 - val_loss: 0.9089 - val_accuracy: 0.6296\nEpoch 12/20\n7/7 [==============================] - 3s 397ms/step - loss: 0.6962 - accuracy: 0.7385 - val_loss: 0.8953 - val_accuracy: 0.6593\nEpoch 13/20\n7/7 [==============================] - 3s 403ms/step - loss: 0.7212 - accuracy: 0.7100 - val_loss: 0.8874 - val_accuracy: 0.6481\nEpoch 14/20\n7/7 [==============================] - 3s 475ms/step - loss: 0.6468 - accuracy: 0.7646 - val_loss: 0.9052 - val_accuracy: 0.6333\nEpoch 15/20\n7/7 [==============================] - 3s 387ms/step - loss: 0.5971 - accuracy: 0.7807 - val_loss: 0.9146 - val_accuracy: 0.6963\nEpoch 16/20\n7/7 [==============================] - 3s 425ms/step - loss: 0.6100 - accuracy: 0.7745 - val_loss: 0.8187 - val_accuracy: 0.6963\nEpoch 17/20\n7/7 [==============================] - 4s 527ms/step - loss: 0.5526 - accuracy: 0.7943 - val_loss: 0.8895 - val_accuracy: 0.6593\nEpoch 18/20\n7/7 [==============================] - 3s 437ms/step - loss: 0.4607 - accuracy: 0.8426 - val_loss: 0.7720 - val_accuracy: 0.7259\nEpoch 19/20\n7/7 [==============================] - 3s 390ms/step - loss: 0.4135 - accuracy: 0.8513 - val_loss: 0.8341 - val_accuracy: 0.6778\nEpoch 20/20\n7/7 [==============================] - 3s 401ms/step - loss: 0.3963 - accuracy: 0.8612 - val_loss: 0.8153 - val_accuracy: 0.7222\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7ff2a0098a90>"},"metadata":{}}]},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model 3:Conv1D with 2 LSTM layers\n","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM\n\nnum_classes=11\n\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, input_shape=(256,1)))\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\nmodel.add(LSTM(264,  return_sequences=True,recurrent_dropout=0.5))\nmodel.add(LSTM(128, return_sequences=True,recurrent_dropout=0.5))\n\n#model.add(LSTM(64,return_sequences=True))  ,recurrent_dropout=0.3\n#model.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax')) \n\nmodel.compile(loss= 'sparse_categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n\nprint(\"done\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T23:28:52.382783Z","iopub.execute_input":"2021-11-27T23:28:52.383039Z","iopub.status.idle":"2021-11-27T23:28:52.641829Z","shell.execute_reply.started":"2021-11-27T23:28:52.383009Z","shell.execute_reply":"2021-11-27T23:28:52.641018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the models","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using Early Stopping to avoid overfitiitng\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n\nmodel.fit(Trainx, Trainy,\n          epochs=20,\n          batch_size= 128,validation_data=(Valx, Valy),callbacks=[callback])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T23:28:55.977849Z","iopub.execute_input":"2021-11-27T23:28:55.9784Z","iopub.status.idle":"2021-11-27T23:31:55.550623Z","shell.execute_reply.started":"2021-11-27T23:28:55.978358Z","shell.execute_reply":"2021-11-27T23:31:55.549937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T23:32:11.763227Z","iopub.execute_input":"2021-11-27T23:32:11.763478Z","iopub.status.idle":"2021-11-27T23:32:12.169013Z","shell.execute_reply.started":"2021-11-27T23:32:11.76345Z","shell.execute_reply":"2021-11-27T23:32:12.168335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate Predictions and CSV File","metadata":{}},{"cell_type":"code","source":"predicted = model.predict(X_test)\npredicted_classes = predicted.argmax(axis=-1)\npredicted_classes","metadata":{"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([ 0,  0,  0,  9,  8,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  7,  0,  0,  0,  7,  0,  1,  1,  1,  1,  1,  1,  1,  3,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  3,  2,  2,  2,\n        2,  2,  2,  2,  5,  2,  2,  2,  2,  2,  2,  2,  6,  2,  2,  2,  2,\n        2,  2,  2,  2,  3,  3,  4,  1,  1,  8, 10,  4,  3,  3,  3,  3,  3,\n       10,  3,  3,  3,  3,  3,  6,  6, 10,  6,  6,  3,  3,  4,  3,  3,  4,\n        4,  3,  7,  4,  4,  3,  3,  0,  3,  4,  4,  4,  4,  6,  4,  3,  4,\n        4,  6,  5,  5,  6,  5,  5,  5,  5,  5,  5,  5,  2,  2,  5,  5,  5,\n        6,  2,  5,  5,  5,  6,  5,  5,  6,  6,  6,  6,  6,  6,  6,  5,  6,\n        6,  6,  6,  6,  6,  6,  2,  6,  6,  6,  6,  6,  6,  6,  6,  8,  8,\n        7,  7,  7,  7,  7,  7,  7,  7,  6,  0,  6, 10,  7,  7,  7,  7,  7,\n        7,  7,  7,  7,  7, 10,  8, 10,  8,  7,  8,  8,  8,  7,  7,  8,  7,\n        0,  0,  0,  0,  8,  0,  7,  7,  8,  7,  7,  7, 10,  9,  9, 10,  9,\n        9, 10,  6, 10,  9,  9,  9,  9,  9,  9,  2, 10,  0,  7,  9,  9,  9,\n        9,  9,  6, 10, 10, 10, 10,  1, 10,  3, 10,  3, 10, 10, 10, 10, 10,\n        3,  3, 10,  3,  2,  9,  7,  8, 10])"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(1):\n    print(\"X=%s, Predicted=%s\" % (X_test[i], predicted_classes[i]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids= list(range(0, 264))\nsubmission = pd.DataFrame({'id':ids,'label':predicted_classes})\nsubmission.to_csv(\"sampleSubmissionGRU.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}